{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efe169f9",
   "metadata": {},
   "source": [
    "# Text Search with TF-IDF\n",
    "\n",
    "For some people exposed to AI, it's the first time they've seen what automation is capable of.\n",
    "Automation has been around for a lot longer than AI, though, and programmers have used it to solve a lot of problems.\n",
    "\n",
    "Given a structured document and a query, produce TF-IDF vectors for the\n",
    "document's text and find the section that's most similar to the query.\n",
    "\n",
    "## Definitions\n",
    "\n",
    "* TF-IDF: term-frequency times inverse document-frequency\n",
    "\n",
    "## Research and Tooling\n",
    "\n",
    "* https://medium.com/@yassineerraji/understanding-textrank-a-deep-dive-into-graph-based-text-summarization-and-keyword-extraction-905d1fb5d266\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting\n",
    "* https://courses.cs.washington.edu/courses/cse373/17au/project3/project3-2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "176bcd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity [[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.14701702]\n",
      " [0.13164123]\n",
      " [0.16353832]\n",
      " [0.13849012]\n",
      " [0.08850576]\n",
      " [0.        ]\n",
      " [0.11962411]\n",
      " [0.        ]\n",
      " [0.0495944 ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.06741477]\n",
      " [0.04413062]\n",
      " [0.0873697 ]\n",
      " [0.04366586]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.06645695]\n",
      " [0.10099313]\n",
      " [0.06270593]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.03664802]\n",
      " [0.13849012]\n",
      " [0.04726681]\n",
      " [0.10315145]\n",
      " [0.04256482]\n",
      " [0.        ]\n",
      " [0.04511777]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.05286617]]\n",
      "Match Index: 11\n",
      "Match Choose Sensor Data Collection to collect data from Vernier sensors including Go Direct sensors, Go!Temp and Go!Motion USB sensors, and wired LabQuest sensors.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "with open('data/chapter-1-structured-content.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "query = \"How do I collect data?\"\n",
    "# query = \"How do I switch to dark mode\"\n",
    "\n",
    "sentences = []\n",
    "for section in data:\n",
    "    sentences.append(section['section_title'])\n",
    "    sentences.append(section['content'])\n",
    "    for subsection in section['subsections']:\n",
    "        sentences.append(subsection['subsection_title'])\n",
    "        sentences.append(subsection['content'])\n",
    "\n",
    "# print(\"Sentences:\", sentences)\n",
    "# print(\"Count:\", len(sentences))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "Y = vectorizer.transform([query])\n",
    "\n",
    "# print(\"X\", X)\n",
    "# print(\"Y\", Y)\n",
    "\n",
    "sim_matrix = cosine_similarity(X, Y)\n",
    "\n",
    "print(\"similarity\", sim_matrix)\n",
    "\n",
    "match_index = np.where(sim_matrix == sim_matrix.max())[0][0]\n",
    "\n",
    "print(\"Match Index:\", match_index)\n",
    "print(\"Match\", sentences[match_index])\n",
    "\n",
    "# nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "\n",
    "# print(\"graph\", nx_graph)\n",
    "\n",
    "# scores = nx.pagerank(nx_graph)\n",
    "\n",
    "# print(\"scores\", scores)\n",
    "\n",
    "# ranked = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "# summary = \" . \".join([s for _, s in ranked[:10]])\n",
    "# print(\"Summary:\", summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
